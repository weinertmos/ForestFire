%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=49336sp\relax

\usepackage[margin=1in,marginparwidth=0.5in]{geometry}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}

\usepackage{multirow}
\usepackage{eqparbox}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.\@ }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing }}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{2}



\title{ForestFire Documentation}
\date{Nov 10, 2017}
\release{1.0.0}
\author{Marlon Weinert}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{index::doc}}



\chapter{\_\_init\_\_}
\label{\detokenize{index:init}}\label{\detokenize{index:module-ForestFire}}\label{\detokenize{index:forestfire}}\index{ForestFire (module)}
\sphinxstyleemphasis{ForestFire} is a Python tool that aims to enhance the performance of machine learning algorithms. 
It utilises the Random Forest algorithm - which is itself a machine learning technique - to determine the 
importance of features in a given set of data and make new predictions which featuresets are most 
likely to yield the best results. 
After building a Random Forest the most promising feature sets are selected and computed. 
The Random Forest is burnt down and a new one is grown until the defined maximum number of forests is reached.
The results can be compared against random search.

\sphinxstyleemphasis{ForestFire} is most usefull in data sets with a number of features greater than 10 where a single run of
a {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}} has a high computational cost. In such data sets the problem arises that some features are
more significant than the rest.
Others may even distort the performance of the underlying {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}} in a negative fashion. 
With a rising number of features a nearly indifinite number of possible selections (= feature sets) emerges.
In those cases ForestFire can help to choose those feature sets that are most promising to yield good results.
By predicting the performance of new feature sets according to their importance in a Random Forest built 
from previous runs it is more likely to find a feature set with a higher performance after a shorter period 
of time than randomly choosing new feature sets.

\sphinxstylestrong{Possible benefits:}
\begin{itemize}
\item {} 
Increase overall precision (higher accuracy / lower Error Rate)

\item {} 
Reduce Computational cost (Finding a good solution earlier)

\item {} 
Gain knowledge about importance of single features

\end{itemize}


\section{How to use}
\label{\detokenize{index:how-to-use}}
In order to use \sphinxstyleemphasis{ForestFire} it is required to provide data in the form of two numpy arrays:
\begin{itemize}
\item {} 
\sphinxstylestrong{X.npy} - contains the values of the features for each data set

\item {} 
\sphinxstylestrong{y.npy} - contains the corresponding performance of those feature sets as a single value

\end{itemize}

The {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}} and the way the raw data is split are configured in two seperate files:
\begin{itemize}
\item {} 
{\hyperref[\detokenize{Importing_Data:import-data}]{\sphinxcrossref{\DUrole{std,std-ref}{import\_data.py}}}} - X and y are loaded from the numpy files in the same folder. 
It is possible to apply data splitting methods here and return the train and test data sets.

\item {} 
{\hyperref[\detokenize{Generate_Database:compute}]{\sphinxcrossref{\DUrole{std,std-ref}{compute.py}}}} - Set up the {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}} that you want to supply with promising selections of 
feature sets generated by \sphinxstyleemphasis{ForestFire}.

\end{itemize}

After \sphinxstyleemphasis{ForestFire} is supplied with the raw Data in X.npy and y.npy, the way this data should be split (import\_data.py)
and the designated {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}} (compute.py) the default setup is complete. 
By executing \sphinxstylestrong{run\_ForestFire.py} the tool can be started with default or adjusted hyperparameters.

\sphinxstyleemphasis{ForestFire} will execute an initial n\_start to set up an internal database. 
From this database single Decision Trees are built and grouped into a Random Forest. 
The Random Forest is evaluated to determine the importance of each feature.
\sphinxstyleemphasis{ForestFire} will next predict the performance of possible new feature sets (chosen both randomly and deliberately).
The two feature sets with the highest predicted performance (mean and variance) are selected, computed by the
original {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}} and added to the database. 
The Random Forest is burnt down and a new one is built, taking into account the two newly generated data points. 
A total number of n\_forests is built.
\sphinxstyleemphasis{ForestFire} will print the current best 5 feature sets as soon as the list of the top 5 feature sets changes.

In \sphinxstyleemphasis{Demo mode}, the performance of \sphinxstyleemphasis{ForestFire} is compared to randomly picking new featuresets.
This can be used to make sure that the algorithm does not only exploit local maxima, but keeps exploring the 
solution space.
The results can be plotted.

Quickstart: \href{https://github.com/weinertmos/ForestFire}{Clone Repository} and run ForestFire-master/Source/ForestFire/run\_ForestFire.py


\chapter{Using ForestFire}
\label{\detokenize{index:get-started}}\label{\detokenize{index:using-forestfire}}

\section{Overview}
\label{\detokenize{Overview:overview}}\label{\detokenize{Overview::doc}}\label{\detokenize{Overview:id1}}
\begin{sphinxadmonition}{note}{Todo}

write a nice Overview
do it in the end when all references are complete
\end{sphinxadmonition}


\subsection{Abbreviations}
\label{\detokenize{Overview:abbreviations}}\phantomsection\label{\detokenize{Overview:dt}}\begin{description}
\item[{DT}] \leavevmode
{\hyperref[\detokenize{Overview:term-decision-tree}]{\sphinxtermref{\DUrole{xref,std,std-term}{Decision Tree}}}}

\end{description}
\phantomsection\label{\detokenize{Overview:rf}}\begin{description}
\item[{RF}] \leavevmode
{\hyperref[\detokenize{Overview:term-random-forest}]{\sphinxtermref{\DUrole{xref,std,std-term}{Random Forest}}}}

\end{description}
\phantomsection\label{\detokenize{Overview:mla}}\begin{description}
\item[{MLA}] \leavevmode
\sphinxstyleemphasis{Machine Learning Algorithm}

\end{description}


\subsection{Glossary}
\label{\detokenize{Overview:glossary}}\begin{description}
\item[{branch\index{branch|textbf}}] \leavevmode\phantomsection\label{\detokenize{Overview:term-branch}}
junction in a {\hyperref[\detokenize{Overview:term-decision-tree}]{\sphinxtermref{\DUrole{xref,std,std-term}{Decision Tree}}}}. Each {\hyperref[\detokenize{Overview:term-node}]{\sphinxtermref{\DUrole{xref,std,std-term}{node}}}} has a true and a false branch leading away from it.

\item[{Decision Tree\index{Decision Tree|textbf}}] \leavevmode\phantomsection\label{\detokenize{Overview:term-decision-tree}}
consists of at least one {\hyperref[\detokenize{Overview:term-node}]{\sphinxtermref{\DUrole{xref,std,std-term}{node}}}} and represents a treelike structure that can be used for classification of new observations

\item[{leaf\index{leaf|textbf}}] \leavevmode\phantomsection\label{\detokenize{Overview:term-leaf}}
Last point of a {\hyperref[\detokenize{Overview:term-branch}]{\sphinxtermref{\DUrole{xref,std,std-term}{branch}}}} in a :term{}`Decision Tree{}`

\item[{node\index{node|textbf}}] \leavevmode\phantomsection\label{\detokenize{Overview:term-node}}
A point in a {\hyperref[\detokenize{Overview:term-decision-tree}]{\sphinxtermref{\DUrole{xref,std,std-term}{Decision Tree}}}} where a decision is made (either true or false)

\item[{pruning\index{pruning|textbf}}] \leavevmode\phantomsection\label{\detokenize{Overview:term-pruning}}
Cutting back {\hyperref[\detokenize{Overview:term-branch}]{\sphinxtermref{\DUrole{xref,std,std-term}{branches}}}} of a {\hyperref[\detokenize{Overview:term-decision-tree}]{\sphinxtermref{\DUrole{xref,std,std-term}{Decision Tree}}}} with little information gain.
See {\hyperref[\detokenize{DT:prune}]{\sphinxcrossref{\DUrole{std,std-ref}{prune}}}}

\item[{Random Forest\index{Random Forest|textbf}}] \leavevmode\phantomsection\label{\detokenize{Overview:term-random-forest}}
Cumulation of {\hyperref[\detokenize{Overview:term-decision-tree}]{\sphinxtermref{\DUrole{xref,std,std-term}{Decision Trees}}}} that can be used for classification of new observations

\end{description}


\subsection{References}
\label{\detokenize{Overview:references}}

\subsection{Utilized Modules}
\label{\detokenize{Overview:utilized-modules}}
The following Modules are imported during the execution of ForestFire:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}  Imports}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{k+kn}{from} \PYG{n+nn}{sklearn} \PYG{k}{import} \PYG{n}{preprocessing}
\PYG{k+kn}{from} \PYG{n+nn}{PIL} \PYG{k}{import} \PYG{n}{Image}\PYG{p}{,} \PYG{n}{ImageDraw}
\end{sphinxVerbatim}


\subsection{About the author}
\label{\detokenize{Overview:about-the-author}}
Information about author.
\phantomsection\label{\detokenize{Overview:blank}}\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.8]{{blank}.jpg}
\label{\detokenize{Overview:blank}}\end{figure}


\section{Import Data}
\label{\detokenize{Importing_Data:import-data}}\label{\detokenize{Importing_Data::doc}}\label{\detokenize{Importing_Data:id1}}
corresponding file: \href{https://github.com/weinertmos/ForestFire/blob/master/source/ForestFire/import\_data.py}{import\_data.py}

In this step the raw data is imported.
It must consist of two numpy arrays \sphinxstylestrong{X} and \sphinxstylestrong{y} which are located in the same directory as \sphinxstyleemphasis{import\_data.py}.
\sphinxstylestrong{X} contains the data sets in rows and the features in columns.
For example, X{[}0:12{]} is the value of the 13th feature in the first data set.
\sphinxstylestrong{y} contains the corresponding result for all data sets in a single column.
It must be of the same length as X.
For example y{[}19{]} is the result of the 20th data set.

After loading the data apply how it should be splitted into train and test data sets and set \sphinxstylestrong{X\_train / X\_test and y\_train / y\_test} accordingly.

\begin{sphinxadmonition}{note}{Note:}
The train/test split in \sphinxstyleemphasis{import\_data.py} will only be done once!
Use it if a fix split is desired.
If a split should be done in every future calculation (e.g. with shufflesplit),
set \sphinxstylestrong{X = X\_test = X\_train and y= y\_test = y\_train} and configure the splitting routine
in the next step ({\hyperref[\detokenize{Generate_Database:compute}]{\sphinxcrossref{\DUrole{std,std-ref}{Generate Database}}}}).
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Important:}
\sphinxstylestrong{Functions used in this chapter}
\index{import\_data() (in module ForestFire.import\_data)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{Importing_Data:ForestFire.import_data.import_data}}\pysiglinewithargsret{\sphinxcode{ForestFire.import\_data.}\sphinxbfcode{import\_data}}{}{}
import the raw data from two numpy arrays.

Import raw data from two numpy arrays X.npy and y.npy. 
Set how train and test data are to be split for fix splits.
Returns train/test splits as well as number of features.

Returns:
\begin{itemize}
\item {} 
X\_test \{np.array\} -- result training data

\item {} 
X\_train \{np.array\} -- feature training data

\item {} 
y\_test \{np.array\} -- result test data

\item {} 
y\_train \{np.array\} -- result training data

\end{itemize}

\end{fulllineitems}

\end{sphinxadmonition}
\phantomsection\label{\detokenize{Importing_Data:blank}}\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.8]{{blank}.jpg}
\label{\detokenize{Importing_Data:blank}}\end{figure}


\section{Generate Database}
\label{\detokenize{Generate_Database:generate-database}}\label{\detokenize{Generate_Database:compute}}\label{\detokenize{Generate_Database::doc}}
corresponding file: \href{https://github.com/weinertmos/ForestFire/blob/master/source/ForestFire/compute.py}{compute.py}

In this step the underlying machine learning algorithm can be configured from scratch or inserted from an existing file.
Required imports can be put at the top of the file.
The default algorithm can be replaced.
As inputs the train / test split data from {\hyperref[\detokenize{Importing_Data:import-data}]{\sphinxcrossref{\DUrole{std,std-ref}{Import Data}}}} can be used.

\begin{sphinxadmonition}{note}{Note:}
If no train / test split has been configured in {\hyperref[\detokenize{Importing_Data:import-data}]{\sphinxcrossref{\DUrole{std,std-ref}{Import Data}}}} it has to be done here.
\end{sphinxadmonition}

The result of the {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}} is stored in the variable \sphinxstyleemphasis{score} and returned to the main file.

\begin{sphinxadmonition}{important}{Important:}
\sphinxstylestrong{Functions used in this chapter}
\index{compute() (in module ForestFire.compute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{Generate_Database:ForestFire.compute.compute}}\pysiglinewithargsret{\sphinxcode{ForestFire.compute.}\sphinxbfcode{compute}}{\emph{X\_train}, \emph{y\_train}, \emph{mask\_sub\_features}, \emph{X\_test}, \emph{y\_test}}{}
Computes a new dataset for the Random Forest with the underlying machine learning algorithm.

Configure your machine learning algorithm here.
Add imports at the top of the file.
If no train / test split is done during import, X\_train and X\_test are equal (y\_train and y\_test as well).
In this case define your own splits with your machine learning algorithm.

Arguments:
\begin{itemize}
\item {} 
X\_train \{np.array\} -- feature training data

\item {} 
y\_train \{np.array\} -- result training data

\item {} 
mask\_sub\_features \{np.array\} -- feature set = dedicated part of all features

\item {} 
X\_test \{np.array\} -- result training data

\item {} 
y\_test \{np.array\} -- result test data

\end{itemize}
\begin{description}
\item[{Returns:}] \leavevmode
score \{np.float64\} -- score of the selected feature set

\end{description}

\end{fulllineitems}

\index{gen\_database() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{Generate_Database:ForestFire.Main.gen_database}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{gen\_database}}{\emph{n\_start}, \emph{X}, \emph{y}, \emph{X\_test}, \emph{y\_test}}{}
Runs the underlying {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}} \sphinxstyleemphasis{n\_start} times to generate a database from which Random Forests can be built.
\begin{description}
\item[{Arguments:}] \leavevmode\begin{itemize}
\item {} 
n\_start \{int\} -- number of times the underlying {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}} is executed

\item {} 
X \{numpy.array\} -- raw data

\item {} 
y \{numpy.array\} -- raw data

\item {} 
X\_test \{numpy.array\} -- test data

\item {} 
y\_test \{numpy.array\} -- test data

\end{itemize}

\item[{Returns:}] \leavevmode
{[}numpy.array{]} -- data set containing feature sets and corresponding results

\end{description}

\end{fulllineitems}

\end{sphinxadmonition}
\phantomsection\label{\detokenize{Generate_Database:blank}}\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.8]{{blank}.jpg}
\label{\detokenize{Generate_Database:blank}}\end{figure}


\section{Execution}
\label{\detokenize{execution:execution}}\label{\detokenize{execution::doc}}\label{\detokenize{execution:id1}}
corresponding file: \href{https://github.com/weinertmos/ForestFire/blob/master/source/ForestFire/run\_ForestFire.py}{run\_ForestFire.py}

After {\hyperref[\detokenize{Importing_Data:import-data}]{\sphinxcrossref{\DUrole{std,std-ref}{importing the raw data}}}} and {\hyperref[\detokenize{Generate_Database:compute}]{\sphinxcrossref{\DUrole{std,std-ref}{configuring the MLA}}}}, ForestFire can be executed.


\subsection{Hyperparameters}
\label{\detokenize{execution:hyperparameters}}\label{\detokenize{execution:id2}}
There is a number of hyperparameters that can be changed or left at default:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Hyperparameters \PYGZsh{}}

\PYG{c+c1}{\PYGZsh{} number of runs before building first Random Forest = number of data points in first RF; minimum = 4, default = 50}
\PYG{c+c1}{\PYGZsh{} adjust according to computational capabilities and demands of the underlying machine learning algorithm}
\PYG{n}{n\PYGZus{}start} \PYG{o}{=} \PYG{l+m+mi}{10}  \PYG{c+c1}{\PYGZsh{} default = 30}
\PYG{c+c1}{\PYGZsh{} if pruning is greater than zero, branches of a Decision Tree will be pruned proportional to pruning value; default = 0}
\PYG{c+c1}{\PYGZsh{} advanced parameter. If set too high, all trees will be cut down to stumps. Increase carefully. Start with values between 0 and 1.}
\PYG{n}{pruning} \PYG{o}{=} \PYG{l+m+mf}{0.4}
\PYG{c+c1}{\PYGZsh{} minimum percentage of Datasets that is used in RF generation; default = 0.2}
\PYG{n}{min\PYGZus{}data} \PYG{o}{=} \PYG{l+m+mf}{0.2}
\PYG{c+c1}{\PYGZsh{} number of forests; minimum=1;  default = 25}
\PYG{c+c1}{\PYGZsh{} adjust according to computational capabilities. For each forest two new computational runs are done. default = 20}
\PYG{n}{n\PYGZus{}forests} \PYG{o}{=} \PYG{l+m+mi}{25}
\end{sphinxVerbatim}

These parameters should be chosen according to computational demand of the {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}}.
It makes sense to start with a small number of runs and increase it carefully.
Pruning is an advanced parameter.
If it is set to high, every single branch will be cut and only a tree stump with a single node is left.
If this parameter is used at all it should be incremented carefully to find a good balance between merging branches and keeping the tree significant.

The following parameters can be left at default since they adapt to the raw data automatically.
But changing them can tweak the performance.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} number of trees that stand in a forest; min = 3; default = number of features * 3}
\PYG{n}{n\PYGZus{}trees} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{default}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{c+c1}{\PYGZsh{} number of deliberately chosen feature sets that get predicted in each forest; default = n\PYGZus{}trees * 5}
\PYG{n}{n\PYGZus{}configs\PYGZus{}biased} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{default}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{c+c1}{\PYGZsh{} number of randomly chosen feature sets that get predicted in each forest; default = n\PYGZus{}configs\PYGZus{}biased * 0.2}
\PYG{n}{n\PYGZus{}configs\PYGZus{}unbiased} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{default}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{c+c1}{\PYGZsh{} sets how aggressively the feature importance changes; default = 0.25}
\PYG{c+c1}{\PYGZsh{} higher values will increase pressure on how often promising features will be selected.}
\PYG{c+c1}{\PYGZsh{} advanced parameter, adjust carefully. If set too high the risk of runnning into local extrema rises.}
\PYG{n}{multiplier\PYGZus{}stepup} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{default}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{c+c1}{\PYGZsh{} number of recent forests that are taken into acount for generating probability of the chosen feature sets default = 4 ? make variable?}
\PYG{n}{seen\PYGZus{}forests} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{default}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{c+c1}{\PYGZsh{} the chosen feature sets default = 4 ? make variable?}

\PYG{c+c1}{\PYGZsh{} weight of the mean in calculating the new probability for selecting future feature sets; default = 0.2}
\PYG{n}{weight\PYGZus{}mean} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{default}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{c+c1}{\PYGZsh{} weight of the gradient in calculating the new probability for selecting future feature sets; default = 0.8}
\PYG{n}{weight\PYGZus{}gradient} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{default}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{c+c1}{\PYGZsh{} which scoring metric should be used in the Decision Tree (available: entropy, giniimpurity and variance); default = entropy}
\PYG{c+c1}{\PYGZsh{} select variance for numerical values in y only}
\PYG{n}{scoref} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{variance}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{c+c1}{\PYGZsh{} set random seed for repeatabilit; comment out if no repeatability is required; default = 1}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Demo Mode \& Plot}
\label{\detokenize{execution:demo-mode-plot}}
In order to compare and plot the performance of ForestFire vs. a randomized search there are two more hyperparameters that can be used:

\begin{sphinxVerbatim}[commandchars=\\\{\}]

\PYG{c+c1}{\PYGZsh{} if true a comparison between the Random Forest driven Search and a random search is done}
\PYG{n}{demo\PYGZus{}mode} \PYG{o}{=} \PYG{k+kc}{True}
\PYG{c+c1}{\PYGZsh{} decide if at the end a plot should be generated , only valid in demo mode}
\end{sphinxVerbatim}

This mode can be usefull when trying to make sure that ForestFire doesn't get caught in a local extremum.
In general ForestFire should always find solutions that are at least as good as a random search - otherwise there is no sense in using it at all - or better.
If that's not the case it might be ``stuck'' at a dominant feature set that seems to perform well, but there are even better feature sets that never get chosen.


\subsection{Output}
\label{\detokenize{execution:output}}
By Executing \href{https://github.com/weinertmos/ForestFire/blob/master/source/ForestFire/run\_ForestFire.py}{run\_ForestFire.py} the algorithm starts.
When a new feature set with good performance (top 5) is found, the current 5 best feature sets and the according performance are printed to the console.
For each feature either 1 or 0 is displayed.
1 means that the underlying {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}} did ``see'' the feature, 0 means this feature was left out

Naturally in the first runs there will be more new best feature sets.
The longer the algorithm continues the harder it gets to find better values.

The importance of a feature can be interpreted by looking at the feature sets that had the best results.
If for example a feature is included in all best feature sets it has a high importance.
If on the other hand a feature is never included, this indicates that the feature is either not important or is even a distortion to the {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}}.


\subsubsection{Example}
\label{\detokenize{execution:example}}
A generic output (with demo mode on) can look like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Starting} \PYG{n}{ForestFire}
\PYG{n}{Loading} \PYG{n}{Raw} \PYG{n}{Data}
\PYG{n}{setting} \PYG{n}{Hyperparameters}
\PYG{n}{Generate} \PYG{n}{Data} \PYG{n}{Base} \PYG{k}{for} \PYG{n}{Random} \PYG{n}{Forest}
\PYG{n}{Starting} \PYG{n}{ForestFire}

\PYG{n}{Building} \PYG{n}{Random} \PYG{n}{Forest} \PYG{n}{Nr}\PYG{o}{.} \PYG{l+m+mi}{1}
\PYG{n}{wrongs}\PYG{p}{:} \PYG{l+m+mi}{9}\PYG{o}{/}\PYG{l+m+mi}{39}
\PYG{n+nb}{max} \PYG{n}{Probability}\PYG{p}{:} \PYG{k+kc}{None}
\PYG{n}{picked} \PYG{n}{biased} \PYG{n}{feature} \PYG{n+nb}{set} \PYG{k}{for} \PYG{n}{mean}
\PYG{n}{picked} \PYG{n}{biased} \PYG{n}{feature} \PYG{n+nb}{set} \PYG{k}{for} \PYG{n}{var}
\PYG{n}{found} \PYG{n}{new} \PYG{n}{best} \PYG{l+m+mi}{5} \PYG{n}{feature} \PYG{n}{sets}\PYG{p}{:} \PYG{p}{[}\PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.74}      \PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}
   \PYG{l+m+mf}{0.72666667}\PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.71}      \PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{0.68666667}\PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}
   \PYG{l+m+mf}{0.67666667}\PYG{p}{]}\PYG{p}{]}

\PYG{n}{Building} \PYG{n}{Random} \PYG{n}{Forest} \PYG{n}{Nr}\PYG{o}{.} \PYG{l+m+mi}{2}
\PYG{n}{wrongs}\PYG{p}{:} \PYG{l+m+mi}{2}\PYG{o}{/}\PYG{l+m+mi}{39}
\PYG{n+nb}{max} \PYG{n}{Probability}\PYG{p}{:} \PYG{k+kc}{None}
\PYG{n}{picked} \PYG{n}{biased} \PYG{n}{feature} \PYG{n+nb}{set} \PYG{k}{for} \PYG{n}{mean}
\PYG{n}{picked} \PYG{n}{unbiased} \PYG{n}{feature} \PYG{n+nb}{set} \PYG{k}{for} \PYG{n}{var}
\PYG{n}{found} \PYG{n}{new} \PYG{n}{best} \PYG{l+m+mi}{5} \PYG{n}{feature} \PYG{n}{sets}\PYG{p}{:} \PYG{p}{[}\PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.74}      \PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}
   \PYG{l+m+mf}{0.72666667}\PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{0.71333333}\PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.71}      \PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.7}       \PYG{p}{]}\PYG{p}{]}

   \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
   \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
   \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}

\PYG{n}{Building} \PYG{n}{Random} \PYG{n}{Forest} \PYG{n}{Nr}\PYG{o}{.} \PYG{l+m+mi}{8}
\PYG{n}{wrongs}\PYG{p}{:} \PYG{l+m+mi}{4}\PYG{o}{/}\PYG{l+m+mi}{39}
\PYG{n+nb}{max} \PYG{n}{Probability}\PYG{p}{:} \PYG{l+m+mf}{0.133463620284}
\PYG{n}{raised} \PYG{n}{multiplier} \PYG{n}{to} \PYG{l+m+mf}{1.03}
\PYG{n}{picked} \PYG{n}{biased} \PYG{n}{feature} \PYG{n+nb}{set} \PYG{k}{for} \PYG{n}{mean}
\PYG{n}{picked} \PYG{n}{biased} \PYG{n}{feature} \PYG{n+nb}{set} \PYG{k}{for} \PYG{n}{var}
\PYG{n}{found} \PYG{n}{new} \PYG{n}{best} \PYG{l+m+mi}{5} \PYG{n}{feature} \PYG{n}{sets}\PYG{p}{:} \PYG{p}{[}\PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{0.76333333}\PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{0.76333333}\PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{0.76333333}\PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{0.74666667}\PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{0.74666667}\PYG{p}{]}\PYG{p}{]}

\PYG{n}{Building} \PYG{n}{Random} \PYG{n}{Forest} \PYG{n}{Nr}\PYG{o}{.} \PYG{l+m+mi}{9}
\PYG{n}{wrongs}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{o}{/}\PYG{l+m+mi}{39}
\PYG{n+nb}{max} \PYG{n}{Probability}\PYG{p}{:} \PYG{l+m+mf}{0.16963581418}
\PYG{n}{picked} \PYG{n}{biased} \PYG{n}{feature} \PYG{n+nb}{set} \PYG{k}{for} \PYG{n}{mean}
\PYG{n}{picked} \PYG{n}{biased} \PYG{n}{feature} \PYG{n+nb}{set} \PYG{k}{for} \PYG{n}{var}

\PYG{n}{Building} \PYG{n}{Random} \PYG{n}{Forest} \PYG{n}{Nr}\PYG{o}{.} \PYG{l+m+mi}{10}
\PYG{n}{wrongs}\PYG{p}{:} \PYG{l+m+mi}{2}\PYG{o}{/}\PYG{l+m+mi}{39}

\PYG{n+nb}{max} \PYG{n}{Probability}\PYG{p}{:} \PYG{l+m+mf}{0.130904237306}
\PYG{n}{raised} \PYG{n}{multiplier} \PYG{n}{to} \PYG{l+m+mf}{1.04}
\PYG{n}{picked} \PYG{n}{biased} \PYG{n}{feature} \PYG{n+nb}{set} \PYG{k}{for} \PYG{n}{mean}
\PYG{n}{picked} \PYG{n}{biased} \PYG{n}{feature} \PYG{n+nb}{set} \PYG{k}{for} \PYG{n}{var}

\PYG{n}{ForestFire} \PYG{n}{finished}

\PYG{n}{Generating} \PYG{n}{more} \PYG{n}{randomly} \PYG{n}{selected} \PYG{n}{feature} \PYG{n}{sets} \PYG{k}{for} \PYG{n}{comparison}
\PYG{n}{best} \PYG{l+m+mi}{5} \PYG{n}{feature} \PYG{n}{sets} \PYG{n}{of} \PYG{n}{random} \PYG{n}{selection}\PYG{p}{:} \PYG{p}{[}\PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}
   \PYG{l+m+mf}{0.72666667}\PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}
   \PYG{l+m+mf}{0.72333333}\PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.71}      \PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{0.70333333}\PYG{p}{]}
 \PYG{p}{[} \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{1.}          \PYG{l+m+mf}{0.}          \PYG{l+m+mf}{1.}
   \PYG{l+m+mf}{0.70333333}\PYG{p}{]}\PYG{p}{]}

\PYG{n}{Lowest} \PYG{n}{MSE} \PYG{n}{after} \PYG{l+m+mi}{50} \PYG{n}{random} \PYG{n}{SVM} \PYG{n}{runs}\PYG{p}{:} \PYG{l+m+mf}{0.726666666667}
\PYG{n}{Lowest} \PYG{n}{MSE} \PYG{n}{of} \PYG{n}{ForestFire} \PYG{n}{after} \PYG{l+m+mi}{30} \PYG{n}{initial} \PYG{n}{random} \PYG{n}{runs} \PYG{o+ow}{and} \PYG{l+m+mi}{20} \PYG{n}{guided} \PYG{n}{runs}\PYG{p}{:} \PYG{l+m+mf}{0.763333333333}
\PYG{n}{Performance} \PYG{k}{with} \PYG{n}{ForestFire} \PYG{n}{improved} \PYG{n}{by} \PYG{l+m+mf}{5.04587155963}\PYG{o}{\PYGZpc{}}
\PYG{n}{Execution} \PYG{n}{finished}

\PYG{n}{Found} \PYG{n}{Best} \PYG{n}{value} \PYG{k}{for} \PYG{n}{Random} \PYG{n}{Forest} \PYG{n}{Search} \PYG{n}{after} \PYG{l+m+mi}{30} \PYG{n}{initial} \PYG{n}{runs} \PYG{o+ow}{and} \PYG{l+m+mi}{11}\PYG{o}{/}\PYG{l+m+mi}{20} \PYG{n}{smart} \PYG{n}{runs}
\PYG{n}{Best} \PYG{n}{value} \PYG{k}{with} \PYG{n}{RF}\PYG{p}{:} \PYG{l+m+mf}{0.763333333333}

\PYG{n}{Found} \PYG{n}{Best} \PYG{n}{value} \PYG{k}{for} \PYG{n}{Random} \PYG{n}{Search} \PYG{n}{after} \PYG{l+m+mi}{18} \PYG{n}{random} \PYG{n}{runs}
\PYG{n}{Best} \PYG{n}{value} \PYG{k}{with} \PYG{n}{Random} \PYG{n}{Search}\PYG{p}{:} \PYG{l+m+mf}{0.726666666667}

\PYG{n}{Creating} \PYG{n}{Plots}

\PYG{p}{[}\PYG{n}{Finished} \PYG{o+ow}{in} \PYG{n}{xxx} \PYG{n}{s}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxstylestrong{Interpretation:}
\begin{quote}

In this example ForestFire was able to find the best solution of 76,3\% accuracy after 30 random and 11 guided runs.
Compared to random search accuracy could be improved by \textasciitilde{}5\%.
The best {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}} run did ``see'' all features but the second.

Since Demo mode was turned on at the end a plot is produced:
\end{quote}
\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.2]{{generic_run}.png}
\end{figure}

\begin{sphinxadmonition}{note}{Todo}

no green highlighting in source code
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Important:}
\sphinxstylestrong{Functions used in this chapter}
\index{main\_loop() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{execution:ForestFire.Main.main_loop}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{main\_loop}}{\emph{n\_start}, \emph{pruning}, \emph{min\_data}, \emph{n\_forests}, \emph{n\_trees}, \emph{n\_configs\_biased}, \emph{n\_configs\_unbiased}, \emph{multiplier\_stepup}, \emph{seen\_forests}, \emph{weight\_mean}, \emph{weight\_gradient}, \emph{scoref}, \emph{demo\_mode}, \emph{plot\_enable}}{}
Load raw data and Generate database for Random Forest. Iteratively build and burn down new Random Forests, predict the performance of new feature sets and compute two new feature sets per round.

Arguments:
\begin{itemize}
\item {} 
n\_start \{int\} -- number of runs before building first RF = number of data points in first RF; minimum = 4, default = 50

\item {} 
pruning \{float\} -- if greater than zero, branches of a Decision Tree will be pruned proportional to pruning value; default = 0

\item {} 
min\_data \{float\} -- minimum percentage of Datasets that is used in RF generation; default = 0.2

\item {} 
n\_forests \{int\} -- number of forests; minimum=1;  default = 25

\item {} 
n\_trees \{int\} -- \# number of trees that stand in a forest; min = 3; default = number of features x 3 x

\item {} 
n\_configs\_biased \{int\} -- \# number of deliberately chosen feature sets that get predicted in each forest; default = n\_trees x 5

\item {} 
n\_configs\_unbiased \{int\} -- \# number of randomly chosen feature sets that get predicted in each forest; default = n\_configs\_biased x0.2

\item {} 
multiplier\_stepup \{float\} -- \# sets how aggressively the feature importance changes; default = 0.25

\item {} 
seen\_forests \{int\} -- \# number of recent forests that are taken into acount for generating probability of the chosen feature sets default = 4

\item {} 
weight\_mean \{float\} -- \# weight of the mean in calculating the new probability for selecting future feature sets; default = 0.2

\item {} 
weight\_gradient \{bool\} -- \# weight of the gradient in calculating the new probability for selecting future feature sets; default = 0.8

\item {} 
scoref \{function\} -- \# which scoring metric should be used in the Decision Tree (available: entropy and giniimpurity); default = entropy

\item {} 
demo\_mode bool -- \# if true a comparison between the Random Forest driven Search and a random search is done

\item {} 
plot\_enable bool -- \# decide if at the end a plot should be generated , only possible in demo mode

\end{itemize}

\end{fulllineitems}

\end{sphinxadmonition}
\phantomsection\label{\detokenize{execution:blank}}\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.8]{{blank}.jpg}
\label{\detokenize{execution:blank}}\end{figure}


\chapter{Building and Burning Random Forests}
\label{\detokenize{index:building-and-burning-random-forests}}\label{\detokenize{index:loop}}

\section{Decision Tree}
\label{\detokenize{DT::doc}}\label{\detokenize{DT:decision-tree}}\label{\detokenize{DT:singletree}}
corresponding file: \href{https://github.com/weinertmos/ForestFire/blob/master/source/ForestFire/Main.py}{Main.py}

The principle of building decision trees is based on the implementation of decision trees in \phantomsection\label{\detokenize{DT:id1}}{\hyperref[\detokenize{Overview:collective-intelligence}]{\sphinxcrossref{{[}Collective\_Intelligence{]}}}} by Toby Segaran.


\subsection{Base Class}
\label{\detokenize{DT:base-class}}
At the foundation of the ForestFire algorithm stands the {\hyperref[\detokenize{DT:decisionnode}]{\sphinxcrossref{\DUrole{std,std-ref}{decisionnode class}}}}.
It represents a node in a {\hyperref[\detokenize{Overview:term-decision-tree}]{\sphinxtermref{\DUrole{xref,std,std-term}{DT}}}} at which the decision is made into which branch (true or false) to proceed.
The whole tree is built up of nodes.
Each node itself can contain two more nodes - the true and false branch - which are themselves decisionnodes.
In this way a tree is constructed in which a set of data takes a certain path along the tree to get classified.
At each node it either enters the true or the false branch.
When a branch is reached with no further branches below, this is called a leaf node.
The leaf node contains the results which represent the classification a data set receives.
The results can be a single value - in this case the classification is 100\% this single value.
It can also consist of several values, e.g. value1 with 2 instances and value2 with 1 instance.
The result of this classification is ambiguous, so it is expressed as a probability: the classification is 1/3 value2 and 2/3 value1.


\subsection{Helper Functions}
\label{\detokenize{DT:helper-functions}}\label{\detokenize{DT:help}}
At each node two questions have to be answered:
\begin{itemize}
\item {} \begin{description}
\item[{By which feature (=column) should the next decision be made?}] \leavevmode
The feature that is chosen at the first node should be the one feature that separates the data set in the best possible way. Latter features are of less importance

\end{description}

\item {} 
By which value should the decision be made?

\end{itemize}

To answer those questions the data is iteratively split in every possible way.
This means it is split for every feature and within every feature it is split for every single value.

See {\hyperref[\detokenize{DT:divideset}]{\sphinxcrossref{\DUrole{std,std-ref}{divideset}}}}

Each of the resulting splits has to be evaluted with respect to ``how well'' the split separates the big list into two smaller lists.
For this three evaluation metrics can be chosen from:
\begin{itemize}
\item {} \begin{description}
\item[{{\hyperref[\detokenize{DT:giniimpurity}]{\sphinxcrossref{\DUrole{std,std-ref}{Gini Impurity}}}}}] \leavevmode
``Probability that a randomly placed item will be in the wrong category''

\end{description}

\item {} \begin{description}
\item[{{\hyperref[\detokenize{DT:entropy}]{\sphinxcrossref{\DUrole{std,std-ref}{Entropy}}}}}] \leavevmode
``How mixed is a list''

\end{description}

\item {} \begin{description}
\item[{{\hyperref[\detokenize{DT:variance}]{\sphinxcrossref{\DUrole{std,std-ref}{Variance}}}}}] \leavevmode
``How far apart do the numbers lie''

\end{description}

\end{itemize}

The evaluation metric returns the gini coefficient / entropy / variance of the list that it is presented with.
Both methods need information about how many unique elements are in one list.
See {\hyperref[\detokenize{DT:uniquecounts}]{\sphinxcrossref{\DUrole{std,std-ref}{uniquecounts}}}}.

After a tree is built its width and depth can be examined by {\hyperref[\detokenize{DT:getdepth}]{\sphinxcrossref{\DUrole{std,std-ref}{getdepth}}}} and {\hyperref[\detokenize{DT:getwidth}]{\sphinxcrossref{\DUrole{std,std-ref}{getwidth}}}}.
A tree's depth is the maximum number of decisions that can be made before reaching a leaf node plus 1 (A tree stump that has no branches by definition still has a depth of 1).
A tree's width is the number of leaves it contains, i.e. number of nodes that have entries in their results property.


\subsection{Building a tree}
\label{\detokenize{DT:building-a-tree}}
Starting with a root node and the whole provided data set the {\hyperref[\detokenize{DT:buildtree}]{\sphinxcrossref{\DUrole{std,std-ref}{buildtree}}}} function recursively
loops through the following steps and builds up the tree structure:
\begin{enumerate}
\item {} 
create a decisionnode

\item {} 
calculate score (entropy / gini coefficient / variance) of current list

\item {} 
divide list into every possible split

\item {} 
evaluate each split according to evaluation metric

\item {} 
split the list into true and false branches according to best evaluated split

\item {} 
If no split is better than the current list no split is performed and results are stored, tree is returned

\item {} 
If true and false branches are created, start at 1.

\end{enumerate}

An example tree can look like {\hyperref[\detokenize{DT:treeview}]{\sphinxcrossref{\DUrole{std,std-ref}{this}}}}.
The first node checks if the value of the third column is \textgreater{}= 21.
If yes it continues to the right and checks column 0 if the value is equal to `slashdot'.
If yes the prediction for the new data set will be 50\% None and 50\% Premium since both values have appeared 1 time during trainging/building of the tree.

If the value of column 0 is instead not equal to `slashdot', there is another query at the next node for colum 0 wether it is equal to `google' and so on.
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.8]{{treeview}.jpg}
\caption{Treeview.jpg}\label{\detokenize{DT:treeview}}\label{\detokenize{DT:id2}}\end{figure}


\subsubsection{Pruning a tree}
\label{\detokenize{DT:pruning-a-tree}}
At the deeper levels of a tree there might be splits that further reduce the entropy / gini coefficient / variance of the data, but only to a minor degree.
These further splits are not productive since they make the tree more complex but yield only small improvements.
There are two ways of tackling this problem.

One is to stop splitting the data if the split does not produce a significant reduction in entropy / gini coefficient / variance.
The danger in doing this is that there is a possibility that at an even later split there might be a significant reduction, but the algorithm can not forsee this.
This would lead to an premature stop.

The better way of dealing with the subject of overly complex trees is {\hyperref[\detokenize{DT:prune}]{\sphinxcrossref{\DUrole{std,std-ref}{pruning}}}}.
The pruning approach builds up the whole complex tree and then starts from its leaves going up.
It takes a look at the information gain that is made by the preceding split.
If the gain is lower than a threshold specified by the \sphinxstyleemphasis{pruning} hyperparameter in {\hyperref[\detokenize{execution:execution}]{\sphinxcrossref{\DUrole{std,std-ref}{Execution}}}} it will reunite the two leaves into one single leaf.
This way no meaningful splits are abandoned but complexity can be reduced

In the {\hyperref[\detokenize{DT:treeview}]{\sphinxcrossref{\DUrole{std,std-ref}{above example tree}}}} the rightmost leaf is the only place where pruning might have hapenned.
Before pruning `None' and `Premium' could have been located in separate leaves.
If the information gain from splitting the two was below the defined threshold, those two leaves would get pruned into one single leaf.
Still, only by looking at the finished tree one cannot tell if the tree was pruned or if it has been built this way (meaning that already during building there was no benefit in creating another split).

\begin{sphinxadmonition}{warning}{Warning:}
By default pruning is disabled (set to 0).
A reasonable value for pruning depends on the raw data.
Observe the output for ``wrongs'' on the console.
By default it should be quite small (\textless{}10\% of the total number of trees at most).
Try a value for pruning between 0 and 1 and only increase above 1 if the ``wrongs'' output does not get too big.

A ``wrong'' tree is a tree ``stump'' consisting of only one node.
Such a tree has no informational benefit.

Being an advanced hyperparameter pruning can greatly improve overall results as well as the number of runs it takes to find a good result.
But it also increases the risk of getting stuck in a local extremum or ending up with a lot of tree `stumps' that are useless for further information retrieval.
\end{sphinxadmonition}


\subsection{Classifying new observations}
\label{\detokenize{DT:classifying-new-observations}}
After a {\hyperref[\detokenize{Overview:term-decision-tree}]{\sphinxtermref{\DUrole{xref,std,std-term}{DT}}}} is built new observations can be classified.
This process can vividly be explained by starting at the top node and asking a simple yes or no question about the corresponding feature and value that is stored in the node.
If the answer for the new observastion is yes, the path follows the true branch of the node.
In case of a negated answer the false branch is pursued.

See {\hyperref[\detokenize{DT:treeview}]{\sphinxcrossref{\DUrole{std,std-ref}{Tree Image}}}} as an example. Visually the true branch is on the right hand side of the parent node, the false branch on the left.

The classification of new data is done with the help of the {\hyperref[\detokenize{DT:classify}]{\sphinxcrossref{\DUrole{std,std-ref}{classify function}}}}.

\begin{sphinxadmonition}{note}{Note:}
{\hyperref[\detokenize{DT:classify}]{\sphinxcrossref{\DUrole{std,std-ref}{classify}}}} is also able to handle missing data entries.
In this case both branches are followed and the result is weighted according to the number of entries they contain.
Since the ForestFire algorithm produces its own database from the raw data and the underlying {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}} it is made sure that there are always entries present and the case of missing entries does not come to pass.
\end{sphinxadmonition}


\subsection{Visualizing a tree}
\label{\detokenize{DT:visualizing-a-tree}}
The following functions are for debugging purposes only.

The structure of the tree can be output to the console with the help of {\hyperref[\detokenize{DT:printtree}]{\sphinxcrossref{\DUrole{std,std-ref}{printtree}}}}.

An image of the tree can be created with the {\hyperref[\detokenize{DT:drawtree}]{\sphinxcrossref{\DUrole{std,std-ref}{drawtree}}}} function.
It makes use of {\hyperref[\detokenize{DT:drawnode}]{\sphinxcrossref{\DUrole{std,std-ref}{drawnode}}}}.


\subsection{Storing the tree structure}
\label{\detokenize{DT:storing-the-tree-structure}}
To {\hyperref[\detokenize{RF:random-forest}]{\sphinxcrossref{\DUrole{std,std-ref}{grow a Random Forest from single Decision Trees}}}} there must be a way to store whole trees and their structure in an array.
Unlike {\hyperref[\detokenize{DT:printtree}]{\sphinxcrossref{\DUrole{std,std-ref}{printtree}}}} and {\hyperref[\detokenize{DT:drawtree}]{\sphinxcrossref{\DUrole{std,std-ref}{drawtree}}}} where the tree is printed / drawn recursively by looping through the nodes.

This is done with the help of {\hyperref[\detokenize{DT:pathgen}]{\sphinxcrossref{\DUrole{std,std-ref}{path\_gen}}}} and {\hyperref[\detokenize{DT:pathgen2}]{\sphinxcrossref{\DUrole{std,std-ref}{path\_gen2}}}}.
By examining the last column of the path matrix that is returned by {\hyperref[\detokenize{DT:pathgen}]{\sphinxcrossref{\DUrole{std,std-ref}{path\_gen}}}} all results of the different leaf nodes can be reached.

Another usefull function is {\hyperref[\detokenize{DT:checkpath}]{\sphinxcrossref{\DUrole{std,std-ref}{check\_path}}}}. It takes as input a tree and a result (typically extracted from a path matrix) and checks wether the result is in that tree. This way it is possible to move along the branches of a tree and at each node check if it (still) contains a certain result, e.g. the best result of the whole tree. This is used for determining the importance of features in the following chapter about {\hyperref[\detokenize{RF:random-forest}]{\sphinxcrossref{\DUrole{std,std-ref}{growing a Random Forest}}}}

\begin{sphinxadmonition}{important}{Important:}
\sphinxstylestrong{Functions used in this chapter}
\phantomsection\label{\detokenize{DT:decisionnode}}\index{decisionnode (class in ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.decisionnode}}\pysiglinewithargsret{\sphinxstrong{class }\sphinxcode{ForestFire.Main.}\sphinxbfcode{decisionnode}}{\emph{col=-1}, \emph{value=None}, \emph{results=None}, \emph{tb=None}, \emph{fb=None}}{}
Base class that a decision tree is built of.
\begin{description}
\item[{Keyword Arguments:}] \leavevmode\begin{itemize}
\item {} 
col \{integer\} -- column number = decision criterium for splitting data (default: \{-1\})

\item {} 
value \{integer/float/string\} -- value by which data gets split (default: \{None\})

\item {} 
results \{integer/float/string\} -- if node is an end node (=leaf) it contains the results (default: \{None\})

\item {} 
tb \{decisionnode\} -- next smaller node containing the true branch (default: \{None\})

\item {} 
fb \{decisionnode\} -- next smaller node containing the false branch (default: \{None\})

\end{itemize}

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:divideset}}\index{divideset() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.divideset}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{divideset}}{\emph{rows}, \emph{column}, \emph{value}}{}
splits a data set into two separate sets according to the column and the value that is passed into.

If value is a number the comparison is done with \textless{}= and \textgreater{}=.
If value is not a number the exact value is compared
\begin{description}
\item[{Arguments:}] \leavevmode\begin{itemize}
\item {} 
rows \{list\} -- data set that is split

\item {} 
column\{integer\} -- column by which data gets split

\item {} 
value \{number/string\} -- value by which data gets split

\end{itemize}

\item[{Returns:}] \leavevmode
{[}list{]} -- two listso

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:giniimpurity}}\index{giniimpurity() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.giniimpurity}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{giniimpurity}}{\emph{rows}}{}
Probability that a randomly placed item will be in the wrong category

Calculates the probability of each possible outcome by dividing the number of times that outcome occurs
by the total number of rows in the set.
It then adds up the products of all these probabilities.
This gives the overall chance that a row would be randomly assigned to the wrong outcome.
The higher this probability, the worse the split.
\begin{description}
\item[{Returns:}] \leavevmode
float -- probability of being in the wrong category

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:entropy}}\index{entropy() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.entropy}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{entropy}}{\emph{rows}}{}
Entropy is the sum of p(x)log(p(x)) across all the different possible results --\textgreater{} how mixed is a list

Funciton calculates the frequency of each item (the number of times it appears divided by the total number of rows)
and applies these formulas:
\begin{align*}\!\begin{aligned}
p(i) = frequency(outcome) = \dfrac{count(outcome)}{count(total rows)}\\
Entropy = \sum(p(i)) \cdot  \log(p(i)) \ for \ all \ outcomes\\
\end{aligned}\end{align*}
The higher the entropy, the worse the split.
\begin{description}
\item[{Arguments:}] \leavevmode
rows \{list\} -- list to evaluate

\item[{Returns:}] \leavevmode
{[}float{]} -- entropy of the list

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:variance}}\index{variance() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.variance}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{variance}}{\emph{rows}}{}
Evaluates how close together numerical values lie

Calculates mean and variance for given list
\begin{align*}\!\begin{aligned}
mean = \dfrac{\sum(entries)}{number \ of \ entries}\\
variance = \sum(entry - mean) ^ 2\\
\end{aligned}\end{align*}\begin{description}
\item[{Arguments:}] \leavevmode
rows \{list\} -- list to evaluate

\item[{Returns:}] \leavevmode
number -- variance of the list

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:uniquecounts}}\index{uniquecounts() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.uniquecounts}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{uniquecounts}}{\emph{rows}}{}
evaluate how many unique elements are in a given list
\begin{description}
\item[{Arguments:}] \leavevmode
rows \{list\} -- evaluated list

\item[{Returns:}] \leavevmode
integer -- number of unique elements

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:getdepth}}\index{getdepth() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.getdepth}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{getdepth}}{\emph{tree}}{}
returns the maximum number of consecutive nodes
\begin{description}
\item[{Arguments:}] \leavevmode
tree \{decisionnode\} -- tree to examine

\item[{Returns:}] \leavevmode
number -- maximum number of consecutive nodes

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:getwidth}}\index{getwidth() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.getwidth}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{getwidth}}{\emph{tree}}{}
returns the number of leaves = endnodes in the tree
\begin{description}
\item[{Arguments:}] \leavevmode
tree \{decisionnode\} -- tree to examine

\item[{Returns:}] \leavevmode
number -- number of endnodes

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:buildtree}}\index{buildtree() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.buildtree}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{buildtree}}{\emph{rows}, \emph{scoref}}{}
recursively builds decisionnode objects that form a decision tree

At each node the best possible split is calculated (depending on the evaluation metric).
If no further split is neccessary the remaining items and their number of occurence
are written in the results property.
\begin{description}
\item[{Arguments:}] \leavevmode
rows \{list\} -- dataset from which to build the tree
scoref \{function\} -- evaluation metric (entropy / gini coefficient)

\item[{Returns:}] \leavevmode
decisionnode -- either two decisionnodes for true and false branch or one decisionnode with results (leaf node)

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:prune}}\index{prune() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.prune}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{prune}}{\emph{tree}, \emph{mingain}}{}
prunes the leaves of a tree in order to reduce complexity

By looking at the information gain that is achieved by splitting data further and further and checking if
it is above the mingain threshold, neighbouring leaves can be collapsed to a single leaf.
\begin{description}
\item[{Arguments:}] \leavevmode
tree \{decisionnode\} -- tree that gets pruned
mingain \{number\} -- threshold for pruning

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:printtree}}\index{printtree() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.printtree}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{printtree}}{\emph{tree}, \emph{indent=' `}}{}
prints out the tree on the command line
\begin{description}
\item[{Arguments:}] \leavevmode
tree \{decisionnode\} -- tree that gets printed

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:drawtree}}\index{drawtree() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.drawtree}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{drawtree}}{\emph{tree}, \emph{jpeg='tree.jpg'}}{}
visualization of the tree in a jpeg
\begin{description}
\item[{Arguments:}] \leavevmode
tree \{decisionnode\} -- tree to draw

\item[{Keyword Arguments:}] \leavevmode
jpeg \{str\} -- Name of the .jpg (default: \{`tree.jpg'\})

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:drawnode}}\index{drawnode() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.drawnode}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{drawnode}}{\emph{draw}, \emph{tree}, \emph{x}, \emph{y}}{}
Helper Function for drawtree, draws a single node
\begin{description}
\item[{Arguments:}] \leavevmode
draw \{img\} -- node to be drawn
tree \{decisionnode\} -- tree that the node belongs to
x \{number\} -- x location
y \{number\} -- y location

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:classify}}\index{classify() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.classify}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{classify}}{\emph{observation}, \emph{tree}}{}
takes a new data set that gets classified and the tree that determines the classification and returns the estimated result.
\begin{description}
\item[{Arguments:}] \leavevmode
observation \{numpy.array\} -- the new data set that gets classified, e.g. test data set
tree \{decisionnode\} -- tree that observation gets classified in

\item[{Returns:}] \leavevmode
data -- expected result

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:pathgen}}\index{path\_gen() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.path_gen}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{path\_gen}}{\emph{tree}}{}
Create a path Matrix which contains the structure of the tree. Calls path\_gen2 to do so.
\begin{description}
\item[{Arguments:}] \leavevmode
tree \{decisionnode\} -- tree of which the data structure is stored

\item[{Returns:}] \leavevmode
numpy.array -- data structure of the tree, NaN means there is no more branch

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:pathgen2}}\index{path\_gen2() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.path_gen2}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{path\_gen2}}{\emph{tree}, \emph{width}, \emph{depth}, \emph{path}, \emph{z2}, \emph{z1}}{}
Create a path Matrix which contains the structure of the tree.

creates a matrix `path' that represents the structure of the tree and the decisions made at each node, last column contains the average MSE at that leaf
the sooner a feature gets chosen as a split feature the more important it is (the farther on the left it appears in path matrix)
order that leaves are written in (top to bottom): function will crawl to the rightmost leaf first (positive side), then jump back up one level and move one step to the left (loop)
\begin{description}
\item[{Arguments:}] \leavevmode
tree \{decisionnode\} -- tree of which the data structure is stored
width \{int\} -- width of the tree
depth \{int\} -- depth of the tree
path \{{[}type{]}\} -- current path matrix, gets updated during function calls
z2 \{int\} -- control variable for current depth
z1 \{int\} -- control variable for current width

\item[{Returns:}] \leavevmode
numpy.array -- the structure of the tree

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:checkpath}}\index{check\_path() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{DT:ForestFire.Main.check_path}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{check\_path}}{\emph{tree}, \emph{result}}{}
Check if a tree contains MSE\_min (= True) or not (= False)
\begin{description}
\item[{Arguments:}] \leavevmode
tree \{decisionnode\} -- tree that gets searched for result
result \{data\} -- result that the tree is searched for

\item[{Returns:}] \leavevmode
bool -- True if result is in the tree, false if not

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{DT:blank}}\end{sphinxadmonition}
\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.8]{{blank}.jpg}
\label{\detokenize{DT:blank}}\end{figure}


\section{Random Forest}
\label{\detokenize{RF:random-forest}}\label{\detokenize{RF::doc}}\label{\detokenize{RF:id1}}
corresponding file: \href{https://github.com/weinertmos/ForestFire/blob/master/source/ForestFire/Main.py}{Main.py}


\subsection{Why is a single Decision Tree not enough?}
\label{\detokenize{RF:why-is-a-single-decision-tree-not-enough}}
A single Decision Tree is already a fully fledged classifier that can be used to determine which features are of more importance than the rest.
The higher up in the hirarchy of the tree a feature stands the more decisive it is with regard to how well it splits the data in two separate lists.
The feature at the top node of a tree can be considered the most important one, a feature that appears on the lower levels is not as importnant.
Consequently a feature that is not at all appearing in the tree is even less important - it is even possible it distorts performance of the {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}}.
As a consequence it might be reasonable to leave features with little importance out of the {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}} and only present it with the important ones.

Applied to a convenient data set this approach can work.
But there are challenges that arise in most real world data sets.
The data can be clustered, i.e. a number of subsequent data sets might follow a certain pattern that is overlooked by a single Decision Tree because it is presented with all the data sets combined.
In addition a single tree that sees all features of the data set tends to be biased towards the most dominant features.

A logical implication to these two challenges is to introduce randomness:
\begin{itemize}
\item {} 
present a single tree with only a random subset of all data sets

\item {} 
present a single tree with only a random subset of all features

\end{itemize}

This will reduce the bias of the tree, but increase its variance.
By building multiple trees and averaging their results the variance can again be reduced.
The term for this construct is {\hyperref[\detokenize{Overview:term-random-forest}]{\sphinxtermref{\DUrole{xref,std,std-term}{Random Forest}}}}.


\subsection{Growing a Random Forest}
\label{\detokenize{RF:growing-a-random-forest}}
In {\hyperref[\detokenize{RF:buildforest}]{\sphinxcrossref{\DUrole{std,std-ref}{buildforest}}}} the Random Forest is built according to the following steps:
\begin{enumerate}
\item {} 
select random data and feature sets from the {\hyperref[\detokenize{Generate_Database:compute}]{\sphinxcrossref{\DUrole{std,std-ref}{generated Database}}}}

\item {} 
build a single tree with ``limited view''

\item {} 
{\hyperref[\detokenize{Overview:term-pruning}]{\sphinxtermref{\DUrole{xref,std,std-term}{prune}}}} the tree (if {\hyperref[\detokenize{execution:hyperparameters}]{\sphinxcrossref{\DUrole{std,std-ref}{enabled}}}})

\item {} 
reward features that lead to the best result

\item {} 
punish features that don't lead to the best result

\item {} 
Build next Tree

\end{enumerate}

After a new tree is built the feature importance for the whole Forest is {\hyperref[\detokenize{RF:update-rf}]{\sphinxcrossref{\DUrole{std,std-ref}{updated}}}} according to the number of appearances in the single trees.
The higher up a feature gets selected in a tree the higher it is rated. The punishment for features that don't lead to the best results is weaker than the reward for leading to the best results.
Features that are not included in the tree get neither a positive nor a negative rating.


\subsection{Predicting new feature sets}
\label{\detokenize{RF:predicting-new-feature-sets}}
After the forest is built it can be used to make predictions (see {\hyperref[\detokenize{RF:forest-predict}]{\sphinxcrossref{\DUrole{std,std-ref}{forest\_predict}}}}) about the performance of arbitrary feature sets.
A new feature set candidate gets classified in every single forest.
The results are averaged.
From the vast amount of possible feature sets two different groups of feature sets are considered:
\begin{itemize}
\item {} 
feature sets biased according to the average importance of each feature

\item {} 
entirely randomly chosen feature sets

\end{itemize}

The two {\hyperref[\detokenize{execution:hyperparameters}]{\sphinxcrossref{\DUrole{std,std-ref}{hyperparameters}}}} \sphinxstyleemphasis{n\_configs\_biased} and \sphinxstyleemphasis{n\_configs\_unbiased} determine the amount of feature sets that get tested.
Since predicting takes not much computing capacity this number can safely be set fairly high.

Of all predicted feature sets two are chosen for the next computing runs with the {\hyperref[\detokenize{Overview:mla}]{\sphinxcrossref{\DUrole{std,std-ref}{MLA}}}}. One with a high average (mean) and one with a high variance.

\begin{sphinxadmonition}{important}{Important:}
\sphinxstylestrong{Functions used in this chapter}
\phantomsection\label{\detokenize{RF:buildforest}}\index{buildforest() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{RF:ForestFire.Main.buildforest}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{buildforest}}{\emph{data}, \emph{n\_trees}, \emph{scoref}, \emph{n\_feat}, \emph{min\_data}, \emph{pruning}}{}
Growing the Random Forest

The Random Forest consists of n\_trees. Each tree sees only a subset of the data and a subset of the features.
Important: a tree never sees the original data set, only the performance of the classifying algorithm
For significant conclusions enough trees must be generated in order to gain the statistical benefits that overcome bad outputs
\begin{description}
\item[{Arguments:}] \leavevmode\begin{itemize}
\item {} 
data \{numpy.array\} -- data set the Forest is built upon

\item {} 
n\_trees \{int\} -- number of trees in a Decision tree

\item {} 
scoref \{function\} -- scoring metric for finding new nodes

\item {} 
n\_feat \{int\} -- number of features in data

\item {} 
min\_data \{float\} -- minimum percentage of all data sets that a tree will see

\item {} 
pruning \{bool\} -- pruning enabled (\textgreater{}0) / disabled(=0)

\end{itemize}

\item[{Returns:}] \leavevmode\begin{itemize}
\item {} 
RF -- importances of single features in the forest

\item {} 
Prob\_current -- importance of the features in the forest

\item {} 
trees -- the structure of the single trees the forest consists of

\end{itemize}

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{RF:update-rf}}\index{update\_RF() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{RF:ForestFire.Main.update_RF}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{update\_RF}}{\emph{RF}, \emph{path}, \emph{tree}, \emph{rand\_feat}}{}
for each tree the features that lead to the leaf with the lowest Error will get rewarded
Features that don't lead to the leaf with the lowest Error will get punished (only by 20\% of the reward)

RF gets updated after a new tree is built and thus contains the cummulation of all
feature appearences in the whole forest
\begin{description}
\item[{Arguments:}] \leavevmode\begin{itemize}
\item {} 
RF \{dict\} -- dictionary that counts occurrence / absence of different features

\item {} 
path \{numpy.array\} -- structure of the current tree

\item {} 
tree \{decisionnode\} -- tree that gets examined

\item {} 
rand\_feat \{list\} -- boolean mask of selected features (1 = selected, 0 = not selected)

\end{itemize}

\item[{Returns:}] \leavevmode\begin{itemize}
\item {} 
RF -- updated dictionary that counts occurrence / absence of different features

\end{itemize}

\end{description}

\end{fulllineitems}

\phantomsection\label{\detokenize{RF:forest-predict}}\index{forest\_predict() (in module ForestFire.Main)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{RF:ForestFire.Main.forest_predict}}\pysiglinewithargsret{\sphinxcode{ForestFire.Main.}\sphinxbfcode{forest\_predict}}{\emph{data}, \emph{trees}, \emph{prob}, \emph{n\_configs}, \emph{biased}}{}
predict performance of new feature sets

Predicts biased and unbiased feature sets in the before constructed Random Forest.
\begin{description}
\item[{Arguments:}] \leavevmode\begin{itemize}
\item {} 
data \{numpy.array\} -- contains all previous computing runs

\item {} 
trees \{decisionnodes\} -- the trees that make up the Random Forest

\item {} 
prob \{array of floats\} -- probability that a feature gets chosen into a feature set

\item {} 
n\_configs \{int\} -- number of feature sets to be generated

\item {} 
biased \{bool\} -- true for biased feature selection, false for unbiased feature selection

\end{itemize}

\item[{Returns:}] \leavevmode\begin{itemize}
\item {} 
best mean -- highest average of all predicted feature sets

\item {} 
best feature set mean -- corresponding boolean list of features (0=feature not chosen, 1=feature chosen)

\item {} 
best var -- highest variance of all predicted feature sets

\item {} 
best feature set var -- corresponding boolean list of features (0=feature not chosen, 1=feature chosen)

\end{itemize}

\end{description}

\end{fulllineitems}

\end{sphinxadmonition}
\phantomsection\label{\detokenize{RF:blank}}\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.8]{{blank}.jpg}
\label{\detokenize{RF:blank}}\end{figure}


\section{Update Database}
\label{\detokenize{Update_Database:update-database}}\label{\detokenize{Update_Database::doc}}
\begin{sphinxadmonition}{important}{Important:}
\sphinxstylestrong{Functions used in this chapter}
\end{sphinxadmonition}


\chapter{Evaluation Mode}
\label{\detokenize{index:demo}}\label{\detokenize{index:evaluation-mode}}

\section{Evaluation}
\label{\detokenize{Evaluation:evaluation}}\label{\detokenize{Evaluation::doc}}
\begin{sphinxadmonition}{important}{Important:}
\sphinxstylestrong{Functions used in this chapter}
\end{sphinxadmonition}


\section{Plot}
\label{\detokenize{Plot:plot}}\label{\detokenize{Plot::doc}}
\begin{sphinxadmonition}{important}{Important:}
\sphinxstylestrong{Functions used in this chapter}
\end{sphinxadmonition}


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}

\sphinxstylestrong{Remaining Todos:}

\begin{sphinxadmonition}{note}{Todo}

write a nice Overview
do it in the end when all references are complete
\end{sphinxadmonition}

(The {\hyperref[\detokenize{Overview:index-0}]{\sphinxcrossref{\sphinxstyleemphasis{original entry}}}} is located in /Users/Dandelo/CloudStation\_Marlon/Energietechnik/Studienarbeit/Sphinx/ForestFire/source/Overview.rst, line 6.)

\begin{sphinxadmonition}{note}{Todo}

no green highlighting in source code
\end{sphinxadmonition}

(The {\hyperref[\detokenize{execution:index-0}]{\sphinxcrossref{\sphinxstyleemphasis{original entry}}}} is located in /Users/Dandelo/CloudStation\_Marlon/Energietechnik/Studienarbeit/Sphinx/ForestFire/source/execution.rst, line 195.)

\begin{sphinxadmonition}{note}{Todo}

so so much more...
\end{sphinxadmonition}

(The {\hyperref[\detokenize{index:index-0}]{\sphinxcrossref{\sphinxstyleemphasis{original entry}}}} is located in /Users/Dandelo/CloudStation\_Marlon/Energietechnik/Studienarbeit/Sphinx/ForestFire/source/index.rst, line 70.)


\chapter{Kinderspielplatz}
\label{\detokenize{index:kinderspielplatz}}
es folgt Quatsch mit Sose

Eine Referenz zu getting started: {\hyperref[\detokenize{index:get-started}]{\sphinxcrossref{\DUrole{std,std-ref}{Using ForestFire}}}}

\begin{sphinxadmonition}{note}{Note:}
Eine Notiz! Ha!
\end{sphinxadmonition}

\begin{sphinxadmonition}{warning}{Warning:}
ohhh eine Warnung!
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Todo}

so so much more...
\end{sphinxadmonition}
\begin{itemize}\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item {} 
eintrag 1

\item {} 
eintrag 2

\item {} 
eintrag 3

\item {} 
eintrag 4

\end{itemize}

Wir bauen einen {\hyperref[\detokenize{Overview:dt}]{\sphinxcrossref{\DUrole{std,std-ref}{DT}}}}

\def\sphinxLiteralBlockLabel{\label{\detokenize{index:id1}}}
\sphinxSetupCaptionForVerbatim{oha}
\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{a} \PYG{o}{=} \PYG{l+m+mi}{5}
\end{sphinxVerbatim}
\let\sphinxVerbatimTitle\empty
\let\sphinxLiteralBlockLabel\empty

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{k+kn}{from} \PYG{n+nn}{sklearn} \PYG{k}{import} \PYG{n}{preprocessing}
\PYG{k+kn}{from} \PYG{n+nn}{PIL} \PYG{k}{import} \PYG{n}{Image}\PYG{p}{,} \PYG{n}{ImageDraw}
\PYG{k+kn}{from} \PYG{n+nn}{compute} \PYG{k}{import} \PYG{n}{compute}
\PYG{k+kn}{from} \PYG{n+nn}{import\PYGZus{}data} \PYG{k}{import} \PYG{n}{import\PYGZus{}data}

\PYG{c+c1}{\PYGZsh{} matplotlib.use(\PYGZsq{}TkAgg\PYGZsq{})  \PYGZsh{} set Backend}


\PYG{c+c1}{\PYGZsh{} change settings}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{set\PYGZus{}printoptions}\PYG{p}{(}\PYG{n}{threshold}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{inf}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} print whole numpy array in console}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{seterr}\PYG{p}{(}\PYG{n}{divide}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ignore}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{invalid}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ignore}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} ignore warnings if dividing by zero or NaN}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{style}\PYG{o}{.}\PYG{n}{use}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bmh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Definitions \PYGZsh{}}


\PYG{k}{def} \PYG{n+nf}{gen\PYGZus{}database}\PYG{p}{(}\PYG{n}{n\PYGZus{}start}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Runs the underlying :ref:{}`MLA \PYGZlt{}MLA\PYGZgt{}{}` *n\PYGZus{}start* times to generate a database from which Random Forests can be built.}

\PYG{l+s+sd}{    Arguments:}
\PYG{l+s+sd}{        * n\PYGZus{}start \PYGZob{}int\PYGZcb{} \PYGZhy{}\PYGZhy{} number of times the underlying :ref:{}`MLA \PYGZlt{}MLA\PYGZgt{}{}` is executed}
\PYG{l+s+sd}{        * X \PYGZob{}numpy.array\PYGZcb{} \PYGZhy{}\PYGZhy{} raw data}
\PYG{l+s+sd}{        * y \PYGZob{}numpy.array\PYGZcb{} \PYGZhy{}\PYGZhy{} raw data}
\PYG{l+s+sd}{        * X\PYGZus{}test \PYGZob{}numpy.array\PYGZcb{} \PYGZhy{}\PYGZhy{} test data}
\PYG{l+s+sd}{        * y\PYGZus{}test \PYGZob{}numpy.array\PYGZcb{} \PYGZhy{}\PYGZhy{} test data}

\PYG{l+s+sd}{    Returns:}
\PYG{l+s+sd}{        [numpy.array] \PYGZhy{}\PYGZhy{} data set containing feature sets and corresponding results}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{X\PYGZus{}DT} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{n\PYGZus{}start}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb}{bool}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Prelocate Memory}
    \PYG{c+c1}{\PYGZsh{} print X\PYGZus{}DT}
    \PYG{n}{y\PYGZus{}DT} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{n\PYGZus{}start}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Prelocate Memory}

    \PYG{c+c1}{\PYGZsh{} create SVMs that can only see subset of features}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}start}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} create random mask to select subgroup of features}
        \PYG{n}{mask\PYGZus{}sub\PYGZus{}features} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb}{bool}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Prelocate Memory}
        \PYG{c+c1}{\PYGZsh{} mask\PYGZus{}sub\PYGZus{}data = np.zeros(len(X), dtype=bool)  \PYGZsh{} Prelocate Memory}
        \PYG{c+c1}{\PYGZsh{} selecting features: any number between 1 and all features are selected}
        \PYG{n}{size} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}
        \PYG{n}{rand\PYGZus{}feat} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{size}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} in first run prob is None \PYGZhy{}\PYGZhy{}\PYGZgt{} all features are equally selected, in later runs prob is result of previous RF results}
        \PYG{n}{mask\PYGZus{}sub\PYGZus{}features}\PYG{p}{[}\PYG{n}{rand\PYGZus{}feat}\PYG{p}{]} \PYG{o}{=} \PYG{k+kc}{True}  \PYG{c+c1}{\PYGZsh{} set chosen features to True}

        \PYG{c+c1}{\PYGZsh{} Select Train and Test Data for subgroup}
        \PYG{c+c1}{\PYGZsh{} print X}
        \PYG{n}{X\PYGZus{}sub} \PYG{o}{=} \PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{mask\PYGZus{}sub\PYGZus{}features}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} select only chosen features (still all datasets)}
        \PYG{c+c1}{\PYGZsh{} print X\PYGZus{}sub}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{10}
\PYG{k}{def} \PYG{n+nf}{gen\PYGZus{}database}\PYG{p}{(}\PYG{n}{n\PYGZus{}start}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Runs the underlying :ref:{}`MLA \PYGZlt{}MLA\PYGZgt{}{}` *n\PYGZus{}start* times to generate a database from which Random Forests can be built.}

\PYG{l+s+sd}{    Arguments:}
\PYG{l+s+sd}{        * n\PYGZus{}start \PYGZob{}int\PYGZcb{} \PYGZhy{}\PYGZhy{} number of times the underlying :ref:{}`MLA \PYGZlt{}MLA\PYGZgt{}{}` is executed}
\PYG{l+s+sd}{        * X \PYGZob{}numpy.array\PYGZcb{} \PYGZhy{}\PYGZhy{} raw data}
\PYG{l+s+sd}{        * y \PYGZob{}numpy.array\PYGZcb{} \PYGZhy{}\PYGZhy{} raw data}
\PYG{l+s+sd}{        * X\PYGZus{}test \PYGZob{}numpy.array\PYGZcb{} \PYGZhy{}\PYGZhy{} test data}
\PYG{l+s+sd}{        * y\PYGZus{}test \PYGZob{}numpy.array\PYGZcb{} \PYGZhy{}\PYGZhy{} test data}

\PYG{l+s+sd}{    Returns:}
\PYG{l+s+sd}{        [numpy.array] \PYGZhy{}\PYGZhy{} data set containing feature sets and corresponding results}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{X\PYGZus{}DT} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{n\PYGZus{}start}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb}{bool}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Prelocate Memory}
    \PYG{c+c1}{\PYGZsh{} print X\PYGZus{}DT}
    \PYG{n}{y\PYGZus{}DT} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{n\PYGZus{}start}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Prelocate Memory}

    \PYG{c+c1}{\PYGZsh{} create SVMs that can only see subset of features}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}start}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} create random mask to select subgroup of features}
        \PYG{n}{mask\PYGZus{}sub\PYGZus{}features} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb}{bool}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Prelocate Memory}
        \PYG{c+c1}{\PYGZsh{} mask\PYGZus{}sub\PYGZus{}data = np.zeros(len(X), dtype=bool)  \PYGZsh{} Prelocate Memory}
        \PYG{c+c1}{\PYGZsh{} selecting features: any number between 1 and all features are selected}
        \PYG{n}{size} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}
        \PYG{n}{rand\PYGZus{}feat} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{size}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} in first run prob is None \PYGZhy{}\PYGZhy{}\PYGZgt{} all features are equally selected, in later runs prob is result of previous RF results}
        \PYG{n}{mask\PYGZus{}sub\PYGZus{}features}\PYG{p}{[}\PYG{n}{rand\PYGZus{}feat}\PYG{p}{]} \PYG{o}{=} \PYG{k+kc}{True}  \PYG{c+c1}{\PYGZsh{} set chosen features to True}

        \PYG{c+c1}{\PYGZsh{} Select Train and Test Data for subgroup}
        \PYG{c+c1}{\PYGZsh{} print X}
        \PYG{n}{X\PYGZus{}sub} \PYG{o}{=} \PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{mask\PYGZus{}sub\PYGZus{}features}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} select only chosen features (still all datasets)}
        \PYG{c+c1}{\PYGZsh{} print len(X\PYGZus{}sub[0])}
        \PYG{c+c1}{\PYGZsh{} print X\PYGZus{}sub[0]}

        \PYG{c+c1}{\PYGZsh{} compute subgroup}
        \PYG{c+c1}{\PYGZsh{} print X\PYGZus{}sub}
        \PYG{n}{y\PYGZus{}DT}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{compute}\PYG{p}{(}\PYG{n}{X\PYGZus{}sub}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{mask\PYGZus{}sub\PYGZus{}features}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Save Data}
        \PYG{n}{X\PYGZus{}DT}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mask\PYGZus{}sub\PYGZus{}features}  \PYG{c+c1}{\PYGZsh{} for the Decision Tree / Random Forest the X values are the information about whether an SVM has seen a certain feature or not}
    \PYG{c+c1}{\PYGZsh{} print X\PYGZus{}DT}
    \PYG{c+c1}{\PYGZsh{} print y\PYGZus{}DT}

    \PYG{c+c1}{\PYGZsh{} merge X and y values}
    \PYG{n}{Data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{X\PYGZus{}DT}\PYG{p}{,} \PYG{n}{y\PYGZus{}DT}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} this Dataset goes into the Decision Tree / Random Forest}
    \PYG{k}{return} \PYG{n}{Data}
\end{sphinxVerbatim}
\phantomsection\label{\detokenize{index:blank}}\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.8]{{blank}.jpg}
\label{\detokenize{index:blank}}\end{figure}

\begin{sphinxthebibliography}{Collective_Intelligence}
\bibitem[Collective\_Intelligence]{\detokenize{Collective_Intelligence}}{\phantomsection\label{\detokenize{Overview:collective-intelligence}} 
Collective Intelligence, O'Reilly, ISBN: 978-0-596-52932-1
}
\end{sphinxthebibliography}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{f}
\item {\sphinxstyleindexentry{ForestFire}}\sphinxstyleindexpageref{index:\detokenize{module-ForestFire}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}