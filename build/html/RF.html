<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Random Forest &#8212; ForestFire 1.1.5 documentation</title>
    
    <link rel="stylesheet" href="_static/haiku.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.1.5',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Update Database" href="Update_Database.html" />
    <link rel="prev" title="Decision Tree" href="DT.html" /> 
  </head>
  <body role="document">
      <div class="header" role="banner"><h1 class="heading"><a href="index.html">
          <span>ForestFire 1.1.5 documentation</span></a></h1>
        <h2 class="heading"><span>Random Forest</span></h2>
      </div>
      <div class="topnav" role="navigation" aria-label="top navigation">
      
        <p>
        «&#160;&#160;<a href="DT.html">Decision Tree</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="Update_Database.html">Update Database</a>&#160;&#160;»
        </p>

      </div>
      <div class="content">
        
        
  <div class="section" id="random-forest">
<span id="id1"></span><h1>Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">¶</a></h1>
<p>corresponding file: <a class="reference external" href="https://github.com/weinertmos/ForestFire/blob/master/source/ForestFire/Main.py">Main.py</a></p>
<div class="section" id="why-a-single-decision-tree-is-not-enough">
<h2>Why a single Decision Tree is not enough<a class="headerlink" href="#why-a-single-decision-tree-is-not-enough" title="Permalink to this headline">¶</a></h2>
<p>A single Decision Tree is already a fully fledged classifier that can be used to determine which features are of more importance than the rest.
The higher up in the hirarchy of the tree a feature stands the more decisive it is with regard to how well it splits the data in two separate lists.
The feature at the top node of a tree can be considered the most important one, a feature that appears on the lower levels is not as importnant.
Consequently a feature that is not at all appearing in the tree is even less important - it is even possible it distorts performance of the <a class="reference internal" href="Overview.html#mla"><span class="std std-ref">MLA</span></a>.
As a consequence it might be reasonable to leave features with little importance out of the <a class="reference internal" href="Overview.html#mla"><span class="std std-ref">MLA</span></a> and only present it with the important ones.</p>
<p>Applied to a convenient data set this approach can work.
But there are challenges that arise in most real world data sets.
The data can be clustered, i.e. a number of subsequent data sets might follow a certain pattern that is overlooked by a single Decision Tree because it is presented with all the data sets combined.
In addition a single tree that sees all features of the data set tends to be biased towards the most dominant features.</p>
<p>A logical implication to these two challenges is to introduce randomness:</p>
<blockquote>
<div><ul class="simple">
<li>present a single tree with only a random subset of all data sets</li>
<li>present a single tree with only a random subsmet of all features</li>
</ul>
</div></blockquote>
<p>This will reduce the bias of the tree, but increase its variance.
By building multiple trees and averaging their results the variance can again be reduced.
The term for this construct is <a class="reference internal" href="Overview.html#term-random-forest"><span class="xref std std-term">Random Forest</span></a>.</p>
</div>
<div class="section" id="growing-a-random-forest">
<h2>Growing a Random Forest<a class="headerlink" href="#growing-a-random-forest" title="Permalink to this headline">¶</a></h2>
<p>In <a class="reference internal" href="#buildforest"><span class="std std-ref">buildforest</span></a> the Random Forest is built according to the following steps:</p>
<blockquote>
<div><ol class="arabic simple">
<li>select random data and feature sets from the <a class="reference internal" href="Generate_Database.html#compute"><span class="std std-ref">generated Database</span></a></li>
<li>build a single tree with &#8220;limited view&#8221;</li>
<li><a class="reference internal" href="Overview.html#term-pruning"><span class="xref std std-term">prune</span></a> the tree (if <a class="reference internal" href="execution.html#hyperparameters"><span class="std std-ref">enabled</span></a>)</li>
<li>reward features that lead to the best result</li>
<li>punish features that don&#8217;t lead to the best result</li>
<li>Build next Tree</li>
</ol>
</div></blockquote>
<p>After a new tree is built the feature importance for the whole Forest is <a class="reference internal" href="#update-rf"><span class="std std-ref">updated</span></a> according to the number of appearances in the single trees.
The higher up a feature gets selected in a tree the higher it is rated. The punishment for features that don&#8217;t lead to the best results is weaker than the reward for leading to the best results.
Features that are not included in the tree get neither a positive nor a negative rating.
Instead their probability of getting chosen as a biased feature set in <a class="reference internal" href="#pred-new"><span class="std std-ref">Predicting new feature sets</span></a> is set to zero.</p>
</div>
<div class="section" id="extracting-the-feature-importance-from-a-random-forest">
<h2>Extracting the feature Importance from a Random Forest<a class="headerlink" href="#extracting-the-feature-importance-from-a-random-forest" title="Permalink to this headline">¶</a></h2>
<p>From the so far completed Random Forests the resulting importance of each single feature can be extracted.
The terms &#8220;importance&#8221; and &#8220;probability&#8221; of a feature are used synonymously, since this value will be used for selecting new feature sets in <a class="reference internal" href="#pred-new"><span class="std std-ref">Predicting new feature sets</span></a>.</p>
<p>In <a class="reference internal" href="#update-prob"><span class="std std-ref">update_prob</span></a> the current feature importance / probability is calculated.
Therefore several parameters are taken into account:</p>
<blockquote>
<div><ul class="simple">
<li>seen_forests: Only a fix number of recent Forest is taken into consideration</li>
<li>weight_mean: From the last seen_forest Forests the mean of each feature is calculated and weighed accordingly</li>
<li>weight_gradient: From the last seen_forest Forests the gradient of each feature is calculated and weighed accordingly</li>
</ul>
<ul class="simple" id="multi">
<li>multiplier: each feature probability is potentized by the current multiplier in order to achieve a more distinct distribution of the probabilites</li>
<li>prob_current: the resulting probability for a feature is a combination of its recent trends for both gradient and mean (for details see <a class="reference internal" href="#update-prob"><span class="std std-ref">update_prob</span></a>)</li>
</ul>
</div></blockquote>
<div class="section" id="multiplier-stepup">
<h3>Multiplier Stepup<a class="headerlink" href="#multiplier-stepup" title="Permalink to this headline">¶</a></h3>
<p>The multiplier that is <a class="reference internal" href="#multi"><span class="std std-ref">applied</span></a> as an exponent to all single feature probabilities is a quantity that is scaled dynamically.
Depending on the <a class="reference internal" href="Overview.html#term-raw-data-set"><span class="xref std std-term">Raw data set</span></a> set it is possible that the feature importances in a Random Forest are all very close to the average importance, hence resembling nothing more than a randomly chosen distribution.
In order to avoid this <a class="reference internal" href="Overview.html#term-forestfire"><span class="xref std std-term">ForestFire</span></a> examines the importances of every single feature after a Random Forest is built.
If the highest feature importance does not lie above a certain threshold (default: 2 times the average importance) the multiplier is raised by the <a class="reference internal" href="execution.html#hyperparameters"><span class="std std-ref">hyperparameter multiplier_stepup</span></a>.</p>
</div>
</div>
<div class="section" id="predicting-new-feature-sets">
<span id="pred-new"></span><h2>Predicting new feature sets<a class="headerlink" href="#predicting-new-feature-sets" title="Permalink to this headline">¶</a></h2>
<p>After the forest is built it can be used to make predictions (see <a class="reference internal" href="#forest-predict"><span class="std std-ref">forest_predict</span></a>) about the performance of arbitrary feature sets.
A new feature set candidate gets classified in every single forest.
The results are averaged.
From the vast amount of possible feature sets two different groups of feature sets are considered:</p>
<blockquote>
<div><ul class="simple">
<li>feature sets biased according to the average importance of each feature (prob_current from <a class="reference internal" href="#update-prob"><span class="std std-ref">update_prob</span></a>)</li>
<li>entirely randomly chosen feature sets</li>
</ul>
</div></blockquote>
<p>The two <a class="reference internal" href="execution.html#hyperparameters"><span class="std std-ref">hyperparameters</span></a> <em>n_configs_biased</em> and <em>n_configs_unbiased</em> determine the amount of feature sets that get tested.</p>
<p>For selecting the biased feature sets the probability of choosing a particular feature depends on its rating calculated in <a class="reference internal" href="#buildforest"><span class="std std-ref">buildforest</span></a>.
The unbiased feature sets are chosen randomly.</p>
<p>Every candidate for future computation in the <a class="reference internal" href="Overview.html#mla"><span class="std std-ref">MLA</span></a> gets predicted in every tree that stands in the <a class="reference internal" href="Overview.html#term-random-forest"><span class="xref std std-term">Random Forest</span></a>. The results are incorporated by their average (mean) and variance.</p>
<p>Of all predicted feature sets two are chosen for the next computing run with the <a class="reference internal" href="Overview.html#mla"><span class="std std-ref">MLA</span></a>. One with a high average (mean) and one with a high variance (respectively a combination of both, for details see <a class="reference internal" href="#forest-predict"><span class="std std-ref">forest_predict</span></a>).</p>
<p>If a feature set has already been computed before, it will not be computed again.
Instead its result is copied to the database.</p>
<p>The <a class="reference internal" href="Update_Database.html#update-database"><span class="std std-ref">Updating of the database</span></a> depicts the last step in the ForestFire Loop.</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p><strong>Functions used in this chapter</strong></p>
<span class="target" id="buildforest"></span><dl class="function">
<dt id="ForestFire.Main.buildforest">
<code class="descclassname">ForestFire.Main.</code><code class="descname">buildforest</code><span class="sig-paren">(</span><em>data</em>, <em>n_trees</em>, <em>scoref</em>, <em>n_feat</em>, <em>min_data</em>, <em>pruning</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ForestFire/Main.html#buildforest"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ForestFire.Main.buildforest" title="Permalink to this definition">¶</a></dt>
<dd><p>Growing the Random Forest</p>
<p>The Random Forest consists of n_trees. Each tree sees only a subset of the data and a subset of the features.
Important: a tree never sees the original data set, only the performance of the classifying algorithm
For significant conclusions enough trees must be generated in order to gain the statistical benefits that overcome bad outputs</p>
<dl class="docutils">
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>data {numpy.array} &#8211; data set the Forest is built upon</li>
<li>n_trees {int} &#8211; number of trees in a Decision tree</li>
<li>scoref {function} &#8211; scoring metric for finding new nodes</li>
<li>n_feat {int} &#8211; number of features in data</li>
<li>min_data {float} &#8211; minimum percentage of all data sets that a tree will see</li>
<li>pruning {bool} &#8211; pruning enabled (&gt;0) / disabled(=0)</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="first last simple">
<li>RF &#8211;  dictionary = importances of single features in the forest</li>
<li>prob_current &#8211; single value for importance, used for generating new biased feature sets</li>
<li>trees &#8211; contains all single trees that stand in the Forest</li>
</ul>
</dd>
</dl>
</dd></dl>

<span class="target" id="update-rf"></span><dl class="function">
<dt id="ForestFire.Main.update_RF">
<code class="descclassname">ForestFire.Main.</code><code class="descname">update_RF</code><span class="sig-paren">(</span><em>RF</em>, <em>path</em>, <em>tree</em>, <em>rand_feat</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ForestFire/Main.html#update_RF"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ForestFire.Main.update_RF" title="Permalink to this definition">¶</a></dt>
<dd><p>for each tree the features that lead to the leaf with the lowest Error will get rewarded.
Features that don&#8217;t lead to the leaf with the lowest Error will get punished (only by 20% of
the amount the &#8220;good&#8221; featurtes get rewarded).</p>
<p>RF is a dictionary that gets updated after a new tree is built and thus contains the cummulation of all
feature appearences in the whole forest.</p>
<dl class="docutils">
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>RF {dict} &#8211; dictionary that counts occurrence / absence of different features</li>
<li>path {numpy.array} &#8211; structure of the current tree</li>
<li>tree {decisionnode} &#8211; tree that gets examined</li>
<li>rand_feat {list} &#8211; boolean mask of selected features (1 = selected, 0 = not selected)</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="first last simple">
<li>RF &#8211; updated dictionary that counts occurrence / absence of different features</li>
</ul>
</dd>
</dl>
</dd></dl>

<span class="target" id="update-prob"></span><dl class="function">
<dt id="ForestFire.Main.update_prob">
<code class="descclassname">ForestFire.Main.</code><code class="descname">update_prob</code><span class="sig-paren">(</span><em>Probability</em>, <em>i</em>, <em>weight_mean</em>, <em>weight_gradient</em>, <em>multiplier</em>, <em>seen_forests</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ForestFire/Main.html#update_prob"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ForestFire.Main.update_prob" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the current Importance / Probability of the single features</p>
<p>Based on the probabilities of each feature in past Forests a new current_prob is calculated that takes into
account the mean and the gradient of the prior feature importances.</p>
<dl class="docutils">
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>Probability {numpy array} &#8211; contains Importances of single features for all past Random Forests</li>
<li>i {integer} &#8211; number of current Forest</li>
<li>weight_mean {float} &#8211; weight of the mean in calculating resulting probability</li>
<li>weight_gradient {float} &#8211; weight of the var in calculating resulting probability</li>
<li>multiplier {float} &#8211; exponent for amplifying probabilities</li>
<li>seen_forests {integer} &#8211; number of before built forest that are considered</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd>prob_current &#8211; list of floats representing the calculated aggregation of recent feature importances</dd>
</dl>
</dd></dl>

<span class="target" id="forest-predict"></span><dl class="function">
<dt id="ForestFire.Main.forest_predict">
<code class="descclassname">ForestFire.Main.</code><code class="descname">forest_predict</code><span class="sig-paren">(</span><em>data</em>, <em>trees</em>, <em>prob</em>, <em>n_configs</em>, <em>biased</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ForestFire/Main.html#forest_predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ForestFire.Main.forest_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict performance of new feature sets</p>
<p>Predicts biased and unbiased feature sets in the before constructed Random Forest.
Feature sets are predicted in every single Decision Tree in the Random Forest.
Results are represented as (mean+0.1*var) and (variance+0.1*mean) for each feature set.
The two best feature sets are selected to be sent into the <a class="reference internal" href="Overview.html#mla"><span class="std std-ref">MLA</span></a>.</p>
<dl class="docutils">
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>data {numpy.array} &#8211; contains all previous computing runs</li>
<li>trees {decisionnodes} &#8211; the trees that make up the Random Forest</li>
<li>prob {array of floats} &#8211; probability that a feature gets chosen into a feature set</li>
<li>n_configs {int} &#8211; number of feature sets to be generated</li>
<li>biased {bool} &#8211; true for biased feature selection, false for unbiased feature selection</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="first last simple">
<li>best mean &#8211; highest average of all predicted feature sets</li>
<li>best feature set mean &#8211; corresponding boolean list of features (0=feature not chosen, 1=feature chosen)</li>
<li>best var &#8211; highest variance of all predicted feature sets</li>
<li>best feature set var &#8211; corresponding boolean list of features (0=feature not chosen, 1=feature chosen)</li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="last function">
<dt>
<code class="descclassname">ForestFire.Main.</code><code class="descname">main_loop</code><span class="sig-paren">(</span><em>n_start</em>, <em>pruning</em>, <em>min_data</em>, <em>n_forests</em>, <em>n_trees</em>, <em>n_configs_biased</em>, <em>n_configs_unbiased</em>, <em>multiplier_stepup</em>, <em>seen_forests</em>, <em>weight_mean</em>, <em>weight_gradient</em>, <em>scoref</em>, <em>demo_mode</em>, <em>plot_enable</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ForestFire/Main.html#main_loop"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Load raw data and Generate database for Random Forest. Iteratively build and burn down new Random Forests, predict the performance of new feature sets and compute two new feature sets per round.</p>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>n_start {int} &#8211; number of runs before building first RF = number of data points in first RF; minimum = 4, default = 50</li>
<li>pruning {float} &#8211; if greater than zero, branches of a Decision Tree will be pruned proportional to pruning value; default = 0</li>
<li>min_data {float} &#8211; minimum percentage of Datasets that is used in RF generation; default = 0.2</li>
<li>n_forests {int} &#8211; number of forests; minimum=1;  default = 25</li>
<li>n_trees {int} &#8211; # number of trees that stand in a forest; min = 3; default = number of features x 3 x</li>
<li>n_configs_biased {int} &#8211; # number of deliberately chosen feature sets that get predicted in each forest; default = n_trees x 5</li>
<li>n_configs_unbiased {int} &#8211; # number of randomly chosen feature sets that get predicted in each forest; default = n_configs_biased x0.2</li>
<li>multiplier_stepup {float} &#8211; # sets how aggressively the feature importance changes; default = 0.25</li>
<li>seen_forests {int} &#8211; # number of recent forests that are taken into acount for generating probability of the chosen feature sets default = 4</li>
<li>weight_mean {float} &#8211; # weight of the mean in calculating the new probability for selecting future feature sets; default = 0.2</li>
<li>weight_gradient {bool} &#8211; # weight of the gradient in calculating the new probability for selecting future feature sets; default = 0.8</li>
<li>scoref {function} &#8211; # which scoring metric should be used in the Decision Tree (available: entropy and giniimpurity); default = entropy</li>
<li>demo_mode bool &#8211; # if true a comparison between the Random Forest driven Search and a random search is done</li>
<li>plot_enable bool &#8211; # decide if at the end a plot should be generated , only possible in demo mode</li>
</ul>
</div></blockquote>
</dd></dl>

</div>
<div class="figure align-center" id="blank">
<a class="reference internal image-reference" href="_images/blank.jpg"><img alt="treeview.jpg" src="_images/blank.jpg" style="width: 1020.0px; height: 1320.0px;" /></a>
</div>
</div>
</div>


      </div>
      <div class="bottomnav" role="navigation" aria-label="bottom navigation">
      
        <p>
        «&#160;&#160;<a href="DT.html">Decision Tree</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="Update_Database.html">Update Database</a>&#160;&#160;»
        </p>

      </div>

    <div class="footer" role="contentinfo">
        &#169; Copyright 2017, Marlon Weinert.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.5.1.
    </div>
  </body>
</html>